# check how cuda works underneath python abstraction

pragma omp parallel // creates team of threads
    if (scalar_expression)
    private (list)
    shared (list)
    default (shared | none) //all threads execute code
    firstprivate (list)
    lastprivate (list)
    reduction (operator: list)
    copyin (list)
    num_threads

# Interesting worksharing constructs
pragma omp for | sections | single

# Interesting thread synchronization constructs
pragma omp barrier | critical | atomic | omp_lock_t lock, omp_init_lock, omp_destroy_lock, omp_set_lock

# Interesting runtime library function
omp_get_num_threads(), omp_get_thread_num(), omp_set_num_threads, omp_set_dynamic(false), omp_set_nested(false)


pragma omp for [clause ...]  //implicit barrier
    // Loop iterations are divided into pieces of size chunk and
    // dynamically scheduled among the threads; when a thread
    // finishes one chunk, it is dynamically assigned another
    private (list)
    firstprivate (list)
    lastprivate (list)
    shared (list)
    schedule (type [,chunk])
    ordered
    reduction (operator: list)
    collapse (n)
    // Threads do not synchronize at the end of the parallel loop.
    nowait

<for_loop>


pragma omp sections [clause ...]
    private (list)
    firstprivate (list)
    lastprivate (list)
    reduction (operator: list)
    nowait

{
 #pragma omp section
 <structured_block>
 #pragma omp section
 <structured_block>
 ...
}

pragma omp single [clause ...]
    private (list)
    firstprivate (list)
    copyprivate (operator: list) // Broadcasts values from single thread to all threads
    nowait

# Tasks how fine grained can we even go in pytorch
pragma omp task
    default (shared|none)
    private (list)
    firstprivate (list)
    shared (list)
    if (logical-expr.)
    final (logical-expr.)
    mergeable
    depend (dependence-type:list)
    priority (priority-value)
    untied

# check how this works in pytorch or if beneficial for image processing!
int histo[MAX_THREADS];
#pragma omp parallel private(i)
{
    int nthreads, myid;
    nthreads = omp_get_num_threads();
    myid = omp_get_thread_num();
    for( i=0; i<N; i++ ) {
        if( A[i]%nthreads == myid ) {
            histo[myid]++;
        }
    }
}


int histo[MAX_THREADS];
#pragma omp parallel private(i)
{
    int nthreads, myid;
    int count=0;
    nthreads = omp_get_num_threads();
    myid = omp_get_thread_num();
    for( i=0; i<N; i++ ) {
        if( A[i]%nthreads == myid ) {
            count++;
        }
    }
    histo[myid]=count;
}


struct mycount
    {int count; char padding[60];};
    struct mycount padhisto[MAXTH];
    #pragma omp parallel private(i)
{
    int nthreads, myid;
    nthreads = omp_get_num_threads();
    myid = omp_get_thread_num();
    for( i=0; i<N; i++ ) {
        if( A[i]%nthreads == myid ) {
            padhisto[myid].count++; }
    }
}   

# ! Very important to prevent unwanted side effects
// OpenMP Data-Sharing Attribute Rules
pragma omp parallel
    Global/static variables: Shared
    Local automatic variables: Private
    Loop iteration variables (in combined parallel for): Private
    Default for other variables: Shared

pragma omp for
    Loop iteration variable: Private
    Variables declared in the loop: Private
    Other variables: Inherit from enclosing parallel region

#pragma omp single
    Variables declared in the single region: Private
    Other variables: Inherit from enclosing parallel region

#pragma omp sections
    Variables declared in the sections: Private
    Other variables: Inherit from enclosing parallel region

#pragma omp task
    Global/static variables: Shared
    Variables declared in the task: Private
    Variables from enclosing context: Firstprivate by default
    Variables explicitly shared in enclosing context: Remain shared

# mpi program structure, compatible with pytorch?
- MPI_INIT
– MPI_FINALIZE
– MPI_COMM_SIZE
– MPI_COMM_RANK
– MPI_SEND
– MPI_RECV

# RMA in MPI? maybe depends how many gpus are available
● One sided
● Remote Memory Access
● MPI_Put and MPI_Get
● only one process (called the “origin”) actively participates in the data transfer
● Guarantees?

Active / Passive Target Synchronization:
● Fence
○ MPI_Win_fence()
○ Collective call over all processes in the group associated with the window object
● PSCW
○ MPI_Win_post()
○ MPI_Win_start()
○ MPI_Win_complete()
○ MPI_Win_wait()
○ More general
○ Can select between exposure and access epochs
● Passive-Target
○ MPI_Win_lock()
○ MPI_Win_unlock()
○ Target does not call any sync routines (hence passive)


MPI Parameters (Put, Get, Send/Rec)
● Put/Get:
○ origin_addr - initial address of origin buffer (choice) || Address of the buffer in
which to receive the data
○ origin_count - number of entries in origin buffer (nonnegative integer)
○ origin_datatype datatype of each entry in origin buffer (handle)
○ Target_rank rank of target (nonnegative integer)
○ Target_disp displacement from start of window to target buffer (nonnegative
integer) || displacement from window start to the beginning of the target buffer
(nonnegative integer)
○ Target_count number of entries in target buffer (nonnegative integer)
○ Target_datatype datatype of each entry in target buffer (handle)
○ Win window object used for communication (handle)
● Send
○ Buf initial address of send buffer (choice)
○ Count number of elements in send buffer (nonnegative integer)
○ Datatype datatype of each send buffer element (handle)
○ Dest rank of destination (integer)
○ Tag message tag (integer)
○ Comm communicator (handle)
● Recv
○ Count maximum number of elements in receive buffer (integer)
○ Datatype datatype of each receive buffer element (handle)
○ Source rank of source (integer)
○ Tag message tag (integer)
○ Comm communicator (handle)

// Collective
MPI_Gather    
MPI_Reduce     
MPI_Scatter     
MPI_Bcast

// Alltoall
MPI_Allgather
MPI_Alltoall
MPI_Allreduce 
MPI_Reduce_scatter

MPI_Scan
MPI_Exscan
MPI_Barrier
// The call blocks until all processes in the group of the communicator comm
// have called MPI_Barrier    

// Modes for sending messages
Standard Mode - MPI_(I)Send (The runtime system decides whether the message is buffered. )
● Buffered mode - MPI_Bsend (user supplies a buffer to the system for its use)
● Ready mode - MPI_Rsend (This mode assumes that an appropriate receive was started)
● Synchronized mode - MPI_Ssend (does not complete until a matching receive has begun)

// One-sided
MPI_Win_create (w/ existing buffer, user-allocated memory exposed as window) or MPI_Win_allocate(_shared) (MPI allocates buffer)
MPI_Win_create_dynamic (create RMA window w/o attached memory, allow user to attach any number of windows) then
MPI_Win_attach
MPI_Win_free

// Atomic one-sided operations:
• MPI_Accumulate, MPI_Get_accumulate (return content of target buffer before accumulation in result_addr, element-wise atomically)
• MPI_Fetch_and_op (Accumulate origin buffer into target buffer using op), MPI_Compare_and_swap (compare_addr with value at target_disp)
